{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset,random_split\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Batch Shape: torch.Size([32, 1, 224, 224])\n",
      "Emotions: tensor([2, 0, 3, 2, 2, 1, 3, 2, 3, 1, 0, 3, 0, 1, 2, 3, 2, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 3, 3, 3, 0, 1, 0, 0])\n",
      "Orientations: tensor([2, 0, 0, 0, 3, 3, 4, 2, 3, 3, 0, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 4, 3,\n",
      "        0, 3, 2, 2, 0, 4, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_directory, transform=None):\n",
    "        self.root_directory = root_directory\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.emotions = []\n",
    "        self.orientations = []\n",
    "\n",
    "        # Define mappings\n",
    "        emotion_mapping = {'neutral': 0, 'sad': 1, 'happy': 2, 'angry': 3}\n",
    "        orientation_mapping = {'up': 0, 'down': 1, 'left': 2, 'right': 3, 'straight': 4}\n",
    "\n",
    "        # Recursively walk through all subdirectories to find images\n",
    "        for subdir, dirs, files in os.walk(root_directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\"_2.pgm\"):\n",
    "                    filepath = os.path.join(subdir, file)\n",
    "                    # Extract label from the filename\n",
    "                    parts = file.split('_')\n",
    "                    orientation = parts[1]  # Get orientation from filename\n",
    "                    emotion = parts[2].split('.')[0]  # Extract the emotion from the filename\n",
    "\n",
    "                    self.images.append(filepath)\n",
    "                    self.emotions.append(emotion_mapping[emotion])\n",
    "                    self.orientations.append(orientation_mapping[orientation])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        emotion = self.emotions[idx]\n",
    "        orientation = self.orientations[idx]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        emotion_tensor = torch.tensor(emotion, dtype=torch.long)\n",
    "        orientation_tensor = torch.tensor(orientation, dtype=torch.long)\n",
    "\n",
    "        return image, emotion_tensor, orientation_tensor\n",
    "\n",
    "# Example usage of the dataset\n",
    "dataset_directory = './faces'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "])\n",
    "\n",
    "dataset = CustomDataset(dataset_directory, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# To test the dataloader\n",
    "for images, emotions, orientations in dataloader:\n",
    "    print(\"Image Batch Shape:\", images.shape)\n",
    "    print(\"Emotions:\", emotions)\n",
    "    print(\"Orientations:\", orientations)\n",
    "    break  # Just to see the first batch\n",
    "\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * 0.8)  # 80% of the dataset for training\n",
    "validation_size = total_size - train_size  # The rest for validation\n",
    "\n",
    "train_dataset, validation_dataset = random_split(dataset, [train_size, validation_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output_dim(self, input_size, kernel_size, kernel_channels, stride, padding, pool_kernel_size):\n",
    "    output_size = (input_size - kernel_size + 2 * padding) // stride + 1\n",
    "    output_size = (output_size - pool_kernel_size + 1) // pool_kernel_size + 1  # Assuming stride of pool_kernel_size\n",
    "    return output_size * output_size * kernel_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskCNN(nn.Module):\n",
    "    def __init__(self, num_emotions, num_orientations):\n",
    "        super(MultiTaskCNN, self).__init__()\n",
    "        # Shared layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.flatten_size = 32 * 56 * 56\n",
    "        \n",
    "        # Task-specific layers\n",
    "        self.fc1_emotion = nn.Linear(self.flatten_size, 128)\n",
    "        self.fc2_emotion = nn.Linear(128, num_emotions)\n",
    "        \n",
    "        self.fc1_orientation = nn.Linear(self.flatten_size, 128)\n",
    "        self.fc2_orientation = nn.Linear(128, num_orientations)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, self.flatten_size)  # Flatten the features\n",
    "        \n",
    "        # Emotion branch\n",
    "        x_emotion = F.relu(self.fc1_emotion(x))\n",
    "        x_emotion = self.fc2_emotion(x_emotion)\n",
    "        \n",
    "        # Orientation branch\n",
    "        x_orientation = F.relu(self.fc1_orientation(x))\n",
    "        x_orientation = self.fc2_orientation(x_orientation)\n",
    "        \n",
    "        return x_emotion, x_orientation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model found, starting training from scratch.\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiTaskCNN(num_emotions=4, num_orientations=5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Path to the model file\n",
    "model_path = 'ckpt.pth'\n",
    "\n",
    "# Check if a trained model .pt file exists\n",
    "if os.path.isfile(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Loaded model weights from:\", model_path)\n",
    "else:\n",
    "    print(\"No model found, starting training from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.7710, Validation Loss: 0.7056\n",
      "Model saved to ckpt.pth after epoch 1 with Validation Loss: 0.7056\n",
      "Epoch 2, Train Loss: 0.6776, Validation Loss: 0.6818\n",
      "Model saved to ckpt.pth after epoch 2 with Validation Loss: 0.6818\n",
      "Epoch 3, Train Loss: 0.6000, Validation Loss: 0.5332\n",
      "Model saved to ckpt.pth after epoch 3 with Validation Loss: 0.5332\n",
      "Epoch 4, Train Loss: 0.6191, Validation Loss: 0.5105\n",
      "Model saved to ckpt.pth after epoch 4 with Validation Loss: 0.5105\n",
      "Epoch 5, Train Loss: 0.4510, Validation Loss: 0.3889\n",
      "Model saved to ckpt.pth after epoch 5 with Validation Loss: 0.3889\n",
      "Epoch 6, Train Loss: 0.4134, Validation Loss: 0.3485\n",
      "Model saved to ckpt.pth after epoch 6 with Validation Loss: 0.3485\n",
      "Epoch 7, Train Loss: 0.3263, Validation Loss: 0.2872\n",
      "Model saved to ckpt.pth after epoch 7 with Validation Loss: 0.2872\n",
      "Epoch 8, Train Loss: 0.2988, Validation Loss: 0.2615\n",
      "Model saved to ckpt.pth after epoch 8 with Validation Loss: 0.2615\n",
      "Epoch 9, Train Loss: 0.2871, Validation Loss: 0.2459\n",
      "Model saved to ckpt.pth after epoch 9 with Validation Loss: 0.2459\n",
      "Epoch 10, Train Loss: 0.2238, Validation Loss: 0.1781\n",
      "Model saved to ckpt.pth after epoch 10 with Validation Loss: 0.1781\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training parameters\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "patience = 2  # Number of epochs to wait after last improvement before stopping the training\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, emotion_labels, orientation_labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        emotion_labels = emotion_labels.to(device)\n",
    "        orientation_labels = orientation_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        emotion_preds, orientation_preds = model(images)\n",
    "        loss_emotion = criterion(emotion_preds, emotion_labels)\n",
    "        loss_orientation = criterion(orientation_preds, orientation_labels)\n",
    "        loss = loss_emotion + loss_orientation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, emotion_labels, orientation_labels in validation_dataloader:\n",
    "            images = images.to(device)\n",
    "            emotion_labels = emotion_labels.to(device)\n",
    "            orientation_labels = orientation_labels.to(device)\n",
    "            emotion_preds, orientation_preds = model(images)\n",
    "            val_loss_emotion = criterion(emotion_preds, emotion_labels)\n",
    "            val_loss_orientation = criterion(orientation_preds, orientation_labels)\n",
    "            val_loss = val_loss_emotion + val_loss_orientation\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Checkpointing based on improvement in validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'ckpt.pth')  # Save the current best model state\n",
    "        print(f\"Model saved to ckpt.pth after epoch {epoch+1} with Validation Loss: {avg_val_loss:.4f}\")\n",
    "        patience_counter = 0  # Reset the patience counter\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter > patience:\n",
    "            print(\"Early stopping due to no improvement in validation loss.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
