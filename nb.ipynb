{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset,random_split\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from faces folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Batch Shape: torch.Size([32, 1, 224, 224])\n",
      "Emotions: tensor([3, 2, 2, 3, 0, 3, 1, 3, 2, 0, 2, 0, 0, 1, 2, 0, 0, 3, 1, 0, 3, 3, 0, 1,\n",
      "        1, 1, 2, 1, 2, 3, 0, 0])\n",
      "Orientations: tensor([0, 0, 2, 2, 2, 0, 3, 2, 3, 2, 3, 4, 4, 2, 4, 2, 0, 0, 2, 4, 4, 4, 3, 4,\n",
      "        0, 3, 0, 3, 4, 4, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_directory, transform=None):\n",
    "        self.root_directory = root_directory\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.emotions = []\n",
    "        self.orientations = []\n",
    "\n",
    "        # Define mappings\n",
    "        emotion_mapping = {'neutral': 0, 'sad': 1, 'happy': 2, 'angry': 3}\n",
    "        orientation_mapping = {'up': 0, 'down': 1, 'left': 2, 'right': 3, 'straight': 4}\n",
    "\n",
    "        # Recursively walk through all subdirectories to find images\n",
    "        for subdir, dirs, files in os.walk(root_directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\"_2.pgm\"):\n",
    "                    filepath = os.path.join(subdir, file)\n",
    "                    # Extract label from the filename\n",
    "                    parts = file.split('_')\n",
    "                    orientation = parts[1]  # Get orientation from filename\n",
    "                    emotion = parts[2].split('.')[0]  # Extract the emotion from the filename\n",
    "\n",
    "                    self.images.append(filepath)\n",
    "                    self.emotions.append(emotion_mapping[emotion])\n",
    "                    self.orientations.append(orientation_mapping[orientation])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        emotion = self.emotions[idx]\n",
    "        orientation = self.orientations[idx]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        emotion_tensor = torch.tensor(emotion, dtype=torch.long)\n",
    "        orientation_tensor = torch.tensor(orientation, dtype=torch.long)\n",
    "\n",
    "        return image, emotion_tensor, orientation_tensor\n",
    "\n",
    "# Example usage of the dataset\n",
    "dataset_directory = './faces'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "])\n",
    "\n",
    "dataset = CustomDataset(dataset_directory, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# To test the dataloader\n",
    "for images, emotions, orientations in dataloader:\n",
    "    print(\"Image Batch Shape:\", images.shape)\n",
    "    print(\"Emotions:\", emotions)\n",
    "    print(\"Orientations:\", orientations)\n",
    "    break  # Just to see the first batch\n",
    "\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * 0.8)  # 80% of the dataset for training\n",
    "validation_size = total_size - train_size  # The rest for validation\n",
    "\n",
    "train_dataset, validation_dataset = random_split(dataset, [train_size, validation_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output_dim(self, input_size, kernel_size, kernel_channels, stride, padding, pool_kernel_size):\n",
    "    output_size = (input_size - kernel_size + 2 * padding) // stride + 1\n",
    "    output_size = (output_size - pool_kernel_size + 1) // pool_kernel_size + 1  # Assuming stride of pool_kernel_size\n",
    "    return output_size * output_size * kernel_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskCNN(nn.Module):\n",
    "    def __init__(self, num_emotions, num_orientations):\n",
    "        super(MultiTaskCNN, self).__init__()\n",
    "        # Shared layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.flatten_size = 32 * 56 * 56\n",
    "        \n",
    "        # Task-specific layers\n",
    "        self.fc1_emotion = nn.Linear(self.flatten_size, 128)\n",
    "        self.fc2_emotion = nn.Linear(128, num_emotions)\n",
    "        \n",
    "        self.fc1_orientation = nn.Linear(self.flatten_size, 128)\n",
    "        self.fc2_orientation = nn.Linear(128, num_orientations)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, self.flatten_size)  # Flatten the features\n",
    "        \n",
    "        # Emotion branch\n",
    "        x_emotion = F.relu(self.fc1_emotion(x))\n",
    "        x_emotion = self.fc2_emotion(x_emotion)\n",
    "        \n",
    "        # Orientation branch\n",
    "        x_orientation = F.relu(self.fc1_orientation(x))\n",
    "        x_orientation = self.fc2_orientation(x_orientation)\n",
    "        \n",
    "        return x_emotion, x_orientation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights from: ckpt.pth\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiTaskCNN(num_emotions=4, num_orientations=5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Path to the model file\n",
    "model_path = 'ckpt.pth'\n",
    "\n",
    "# Check if a trained model .pt file exists\n",
    "if os.path.isfile(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Loaded model weights from:\", model_path)\n",
    "else:\n",
    "    print(\"No model found, starting training from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.0642, Validation Loss: 0.0600\n",
      "Model saved to ckpt.pth after epoch 1 with Validation Loss: 0.0600\n",
      "Epoch 2, Train Loss: 0.0638, Validation Loss: 0.0600\n",
      "Epoch 3, Train Loss: 0.0652, Validation Loss: 0.0600\n",
      "Epoch 4, Train Loss: 0.0665, Validation Loss: 0.0600\n",
      "Early stopping due to no improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training parameters\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "patience = 2  # Number of epochs to wait after last improvement before stopping the training\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, emotion_labels, orientation_labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        emotion_labels = emotion_labels.to(device)\n",
    "        orientation_labels = orientation_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        emotion_preds, orientation_preds = model(images)\n",
    "        loss_emotion = criterion(emotion_preds, emotion_labels)\n",
    "        loss_orientation = criterion(orientation_preds, orientation_labels)\n",
    "        loss = loss_emotion + loss_orientation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, emotion_labels, orientation_labels in validation_dataloader:\n",
    "            images = images.to(device)\n",
    "            emotion_labels = emotion_labels.to(device)\n",
    "            orientation_labels = orientation_labels.to(device)\n",
    "            emotion_preds, orientation_preds = model(images)\n",
    "            val_loss_emotion = criterion(emotion_preds, emotion_labels)\n",
    "            val_loss_orientation = criterion(orientation_preds, orientation_labels)\n",
    "            val_loss = val_loss_emotion + val_loss_orientation\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Checkpointing based on improvement in validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'ckpt.pth')  # Save the current best model state\n",
    "        print(f\"Model saved to ckpt.pth after epoch {epoch+1} with Validation Loss: {avg_val_loss:.4f}\")\n",
    "        patience_counter = 0  # Reset the patience counter\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter > patience:\n",
    "            print(\"Early stopping due to no improvement in validation loss.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model from sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskCNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1_emotion): Linear(in_features=100352, out_features=128, bias=True)\n",
       "  (fc2_emotion): Linear(in_features=128, out_features=4, bias=True)\n",
       "  (fc1_orientation): Linear(in_features=100352, out_features=128, bias=True)\n",
       "  (fc2_orientation): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'ckpt.pth'\n",
    "model = MultiTaskCNN(num_emotions=4, num_orientations=5).to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Image Results:\n",
      "True Emotion: angry, Predicted Emotion: angry\n",
      "True Orientation: up, Predicted Orientation: up\n",
      "--------------------------------------------------------------\n",
      "Sampled Image Results:\n",
      "True Emotion: neutral, Predicted Emotion: neutral\n",
      "True Orientation: straight, Predicted Orientation: straight\n",
      "--------------------------------------------------------------\n",
      "Sampled Image Results:\n",
      "True Emotion: happy, Predicted Emotion: happy\n",
      "True Orientation: left, Predicted Orientation: left\n",
      "--------------------------------------------------------------\n",
      "Sampled Image Results:\n",
      "True Emotion: angry, Predicted Emotion: angry\n",
      "True Orientation: up, Predicted Orientation: up\n",
      "--------------------------------------------------------------\n",
      "Sampled Image Results:\n",
      "True Emotion: neutral, Predicted Emotion: neutral\n",
      "True Orientation: left, Predicted Orientation: left\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "images, emotion_labels, orientation_labels = next(iter(validation_dataloader))\n",
    "\n",
    "# Randomly select one image and label from the batch\n",
    "index = torch.randint(0, images.size(0), (1,)).item()\n",
    "sample_image = images[index].unsqueeze(0).to(device)  # Add batch dimension and send to device\n",
    "true_emotion_label = emotion_labels[index].item()\n",
    "true_orientation_label = orientation_labels[index].item()\n",
    "\n",
    "# Define mappings for printing\n",
    "emotion_mapping = {0: 'neutral', 1: 'sad', 2: 'happy', 3: 'angry'}\n",
    "orientation_mapping = {0: 'up', 1: 'down', 2: 'left', 3: 'right', 4: 'straight'}\n",
    "\n",
    "for _ in range(5):\n",
    "    # Randomly select one image and label from the entire validation set\n",
    "    images, emotion_labels, orientation_labels = next(iter(validation_dataloader))\n",
    "    index = randint(0, images.size(0) - 1)\n",
    "    sample_image = images[index].unsqueeze(0).to(device)  # Add batch dimension and send to device\n",
    "    true_emotion_label = emotion_labels[index].item()\n",
    "    true_orientation_label = orientation_labels[index].item()\n",
    "\n",
    "    # Make a prediction\n",
    "    with torch.no_grad():\n",
    "        emotion_pred, orientation_pred = model(sample_image)\n",
    "        predicted_emotion = torch.argmax(emotion_pred, 1).item()\n",
    "        predicted_orientation = torch.argmax(orientation_pred, 1).item()\n",
    "\n",
    "    # Print results\n",
    "    print(\"Sampled Image Results:\")\n",
    "    print(f\"True Emotion: {emotion_mapping[true_emotion_label]}, Predicted Emotion: {emotion_mapping[predicted_emotion]}\")\n",
    "    print(f\"True Orientation: {orientation_mapping[true_orientation_label]}, Predicted Orientation: {orientation_mapping[predicted_orientation]}\")\n",
    "    print(\"--------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
